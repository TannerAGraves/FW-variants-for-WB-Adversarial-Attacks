{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:38:26.596035Z",
     "start_time": "2024-07-04T17:38:23.333755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from L_calc_new import *"
   ],
   "id": "b3e743c86174004d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Model weights loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\alepa/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models / Datasets",
   "id": "e6b229bce8fccb63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:38:26.605420Z",
     "start_time": "2024-07-04T17:38:26.597041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LeNet():\n",
    "    def __init__(self, weigths_pth = \"models/lenet5_model.pth\"):\n",
    "        self.query_count = 0\n",
    "        self.batch_size = 64\n",
    "        self.num_classes = 10\n",
    "        self.classes = [str(i) for i in range(10)]\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 10\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        self.trained_mdl_pth = weigths_pth\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.requires_denorm = True\n",
    "        self.mean = [0.1307]\n",
    "        self.std = [0.3081]\n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ])\n",
    "        \n",
    "        self.testloader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ])),\n",
    "                batch_size=1, shuffle=True)\n",
    "\n",
    "        self.model = torch_model().to(self.device)\n",
    "        self.model.load_state_dict(torch.load(self.trained_mdl_pth, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        with torch.no_grad(): # note this should not be left on for WB attacks\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in self.test_loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "\n",
    "    def denorm(self, batch):\n",
    "        mean, std = self.mean, self.std\n",
    "        if isinstance(mean, list):\n",
    "            mean = torch.tensor(mean).to(self.device)\n",
    "        if isinstance(std, list):\n",
    "            std = torch.tensor(std).to(self.device)\n",
    "        return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "\n",
    "    def renorm(self, batch):\n",
    "        return transforms.Normalize(tuple(self.mean), tuple(self.std))(batch).detach()\n",
    "\n",
    "    def remake_testloader(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.testloader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ])),\n",
    "                batch_size=1, shuffle=True)\n",
    "        return self.testloader\n",
    "class torch_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(torch_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ],
   "id": "82c963c4c9d18a28",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:39:02.950481Z",
     "start_time": "2024-07-04T17:39:02.943154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class simple_FashionMNIST():\n",
    "    def __init__(self, trained_mdl_pth):\n",
    "        self.query_count = 0\n",
    "        self.trained_mdl_pth = trained_mdl_pth\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Running on {self.device}.\")\n",
    "\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "        self.testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        self.testloader = torch.utils.data.DataLoader(self.testset, batch_size=1, shuffle=False)\n",
    "        self.requires_denorm = False\n",
    "\n",
    "        self.classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.model = BasicCNN().to(self.device)\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "\n",
    "        if os.path.exists(self.trained_mdl_pth):\n",
    "            self.load_model()\n",
    "        else:\n",
    "            print(f\"No model file found at {self.trained_mdl_pth}.\\nTraining...\")\n",
    "            self.train(self.trained_mdl_pth)\n",
    "\n",
    "    def load_model(self, mdl_pth=None):\n",
    "        mdl_pth = self.trained_mdl_pth if mdl_pth is None else mdl_pth\n",
    "        #self.model = BasicCNN(self.num_classes).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(self.trained_mdl_pth))\n",
    "        self.model.eval()\n",
    "        print(\"Model weights loaded successfully\")\n",
    "        return\n",
    "    \n",
    "    def train(self):\n",
    "        num_epochs = 5\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            \n",
    "            for images, labels in self.trainloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                \n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "    def denorm(self, batch):\n",
    "        return batch\n",
    "    def renorm(self, batch):\n",
    "        return batch.detach()\n",
    "    \n",
    "    def remake_testloader(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.testloader = torch.utils.data.DataLoader(self.testset, batch_size=1, shuffle=False)\n",
    "        return self.testloader\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)  # Output: [batch_size, 32, 26, 26]\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3) # Output: [batch_size, 64, 11, 11]\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)  # Flattening: [batch_size, 64*5*5]\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(-1, 64 * 5 * 5) # Flattening\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "id": "27f4934c0d2fcf5c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:39:04.158610Z",
     "start_time": "2024-07-04T17:39:04.153674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet20():\n",
    "    def __init__(self):\n",
    "        self.query_count = 0\n",
    "        self.model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "        self.model.eval()\n",
    "        self.num_classes = 10\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.seed = 42\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.classes = [\n",
    "                        'airplane', \t\t\t\t\t\t\t\t\t\t\n",
    "                        'automobile', \t\t\t\t\t\t\t\t\t\n",
    "                        'bird', \t\t\t\t\t\t\t\t\t\n",
    "                        'cat', \t\t\t\t\t\t\t\t\t\t\n",
    "                        'deer',\t\t\t\t\t\t\t\t\t\t\n",
    "                        'dog', \t\t\t\t\t\t\t\t\t\t\n",
    "                        'frog', \t\t\t\t\t\t\t\t\t\t\n",
    "                        'horse', \t\t\t\t\t\t\t\t\t\t\n",
    "                        'ship', \t\t\t\t\t\t\t\t\t\t\n",
    "                        'truck'\n",
    "                        ]\n",
    "\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.mean = (0.4914, 0.4822, 0.4465)\n",
    "        self.std = (0.2023, 0.1994, 0.2010)\n",
    "        self.testset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                                train=False, \n",
    "                                                download=True,\n",
    "                                                transform=transforms.Compose([\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(self.mean, self.std)\n",
    "                                                ])\n",
    "        )\n",
    "        self.testloader = torch.utils.data.DataLoader(self.testset,\n",
    "                                                batch_size=1, \n",
    "                                                shuffle=True)\n",
    "    \n",
    "    def remake_testloader(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.testloader = torch.utils.data.DataLoader(self.testset,\n",
    "                                                batch_size=1, \n",
    "                                                shuffle=True)\n",
    "        return self.testloader\n",
    "    def denorm(self, batch):\n",
    "        mean, std = self.mean, self.std\n",
    "        mean = torch.tensor(mean).to(self.device)\n",
    "        std = torch.tensor(std).to(self.device)\n",
    "        return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "    \n",
    "    def renorm(self, batch):\n",
    "        return transforms.Normalize(self.mean, self.std)(batch).detach()"
   ],
   "id": "b6ee2dbf10c4a78",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run only ONE of the following three cells",
   "id": "ae89b30396f0e433"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:38:30.982443Z",
     "start_time": "2024-07-04T17:38:30.967924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.LeNet import *\n",
    "target_mdl = LeNet(\"models/lenet_mnist_model.pth\")\n",
    "model = target_mdl.model\n",
    "device = target_mdl.device\n",
    "test_loader = target_mdl.testloader\n",
    "mdl_name='MNIST'"
   ],
   "id": "68ed030d87c34280",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:39:06.366506Z",
     "start_time": "2024-07-04T17:39:06.338467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.simple_FashionMNIST import *\n",
    "target_mdl = simple_FashionMNIST(\"models/simple_FashionMNIST.pth\")\n",
    "model = target_mdl.model\n",
    "device = target_mdl.device\n",
    "test_loader = target_mdl.testloader\n",
    "mdl_name = 'FMNIST'"
   ],
   "id": "577941e7c066d419",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Model weights loaded successfully\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.resNet import ResNet20\n",
    "target_mdl = ResNet20()\n",
    "model = target_mdl.model\n",
    "device = target_mdl.device\n",
    "test_loader = target_mdl.testloader\n",
    "mdl_name = 'ResNet'"
   ],
   "id": "e55626b613476594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attack utils",
   "id": "f775519d11cb4221"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:39:08.185038Z",
     "start_time": "2024-07-04T17:39:08.161588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttackStep:\n",
    "    def __init__(self, method, epsilon, x0_denorm, lmo, stepsize_method=None, momentum=0.8):\n",
    "        self.method = method\n",
    "        self.epsilon = epsilon\n",
    "        self.x0_denorm = x0_denorm\n",
    "        self.stepsize_method = stepsize_method\n",
    "        self.lmo = lmo\n",
    "        self.momentum = momentum\n",
    "        self.m_t_last = None\n",
    "        self.m_t = None\n",
    "        self.S_t = [x0_denorm]\n",
    "        self.A_t = [1]\n",
    "        self.x_t_denorm = None\n",
    "\n",
    "    def step(self, x_t_denorm, x_t_grad):\n",
    "        if self.method == 'fgsm':\n",
    "            return self.fgsm_attack(x_t_denorm, x_t_grad)\n",
    "        elif self.method == 'fw':\n",
    "            return self.fw_step(x_t_denorm, x_t_grad)\n",
    "        elif self.method == 'fw_momentum':\n",
    "            return self.fw_step_momentum(x_t_denorm, x_t_grad, momentum=self.momentum)\n",
    "        elif self.method == 'fw_away':\n",
    "            return self.fw_step_away(x_t_denorm, x_t_grad)\n",
    "        elif self.method == 'fw_away_m':\n",
    "            return self.fw_step_away_m(x_t_denorm, x_t_grad, momentum=self.momentum)\n",
    "        elif self.method == 'fw_pair':\n",
    "            return self.fw_step_pairwise(x_t_denorm, x_t_grad)\n",
    "        elif self.method == 'fw_pair_test':\n",
    "            return self.fw_step_pairwise_test(x_t_denorm, x_t_grad)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        \n",
    "    def fgsm_attack(self, image, data_grad):\n",
    "        # Collect the element-wise sign of the data gradient\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        # Create the perturbed image by adjusting each pixel of the input image\n",
    "        perturbed_image = image + self.epsilon * sign_data_grad\n",
    "        # Adding clipping to maintain [0,1] range\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        # Return the perturbed image\n",
    "        return perturbed_image\n",
    "\n",
    "    def pgd_attack(self, x_t, g_t):\n",
    "        perturbed_image = x_t + self.stepsize_method.get_stepsize(x_t, g_t) * g_t\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        return perturbed_image\n",
    "\n",
    "    def fw_step(self, x_t, g_t):\n",
    "        info = {}\n",
    "        # Use LMO to compute the attack direction\n",
    "        v_t = self.lmo.get(g_t)\n",
    "        d_t = v_t - x_t\n",
    "        self.d_t = d_t\n",
    "        fw_stepsize = self.stepsize_method.get_stepsize(x_t, d_t)\n",
    "        info['stepsize'] = fw_stepsize\n",
    "        perturbed_image = x_t + fw_stepsize * d_t\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        gap_FW = torch.sum(-d_t * g_t).item()\n",
    "        return perturbed_image, gap_FW, info\n",
    "\n",
    "    def fw_step_momentum(self, x_t, g_t,  momentum=0.8):\n",
    "        # alg from attacks.pdf\n",
    "        info = {}\n",
    "        m_t = (1 - momentum) * g_t\n",
    "        if self.m_t_last is not None:\n",
    "            m_t += momentum * self.m_t_last\n",
    "        v_t = self.lmo.get(m_t)\n",
    "        d_t = v_t - x_t\n",
    "\n",
    "        fw_stepsize = self.stepsize_method.get_stepsize(x_t, d_t)\n",
    "\n",
    "        gap_FW = torch.sum(-d_t * g_t).item()\n",
    "        perturbed_image = x_t + fw_stepsize * d_t\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        self.m_t_last = m_t.clone().detach()\n",
    "        return perturbed_image, gap_FW, info\n",
    "\n",
    "    def update_active_away(self, gamma, s_t, v_t_idx, step_type, debug = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gamma (float): stepsize\n",
    "            gamma_max (int): Max stepsize informs when FW step will make S_t singular or AS dropstep\n",
    "            S_t (list(torch.Tensor)): Active set of directions s.t. x_t in conv{S_t}\n",
    "            A_t (list(float)): coefficients corresponding to atoms in S_t. x_t = A_t .* S_t\n",
    "            v_t_idx (int): index of away atom in S_t\n",
    "        \"\"\"\n",
    "        info = {}\n",
    "        debug_info = {}\n",
    "        if step_type == 'FW':\n",
    "            if abs(gamma - 1) < 0.0001:\n",
    "                # drop step\n",
    "                self.S_t = [s_t]\n",
    "                self.A_t = [1]\n",
    "                debug_info['drop_step'] = 'FW'\n",
    "            else:\n",
    "                ## UPDATE S\n",
    "                # need to check if vertex is already in S\n",
    "                diffs = [torch.sum(torch.abs(s_t - s)).item() for s in self.S_t]\n",
    "                min_diff = min(diffs)\n",
    "                arg = np.argmin(diffs)\n",
    "                if min_diff < 0.9*self.epsilon:\n",
    "                    # s_t already in S_t\n",
    "                    s_t_idx = arg\n",
    "                    debug_info['FW_revisit'] = True\n",
    "                else:\n",
    "                    self.S_t.append(s_t)\n",
    "                    self.A_t.append(0.0)\n",
    "                    s_t_idx = -1\n",
    "                #debug_info[\"min_revisit_diff\"] = min_diff\n",
    "                \n",
    "                ## UPDATE ALPHAS\n",
    "                self.A_t = [(1 - gamma) * alpha for alpha in self.A_t]\n",
    "                self.A_t[s_t_idx] += gamma\n",
    "        elif step_type == 'AS':\n",
    "            if False: #gamma >= gamma_max:\n",
    "                # drop step: remove atom and alpha\n",
    "                # logic changed to check if alpha is zeroed (see below)\n",
    "                self.A_t.pop(v_t_idx)\n",
    "                self.S_t.pop(v_t_idx)\n",
    "                debug_info['drop_step'] = 'AS'\n",
    "            else:\n",
    "                ## UPDATE ALPHAS\n",
    "                self.A_t = [(1 + gamma) * alpha for alpha in self.A_t]\n",
    "                self.A_t[v_t_idx] -= gamma\n",
    "        else:\n",
    "            raise Exception(\"Step must be FW or AS\")\n",
    "        \n",
    "        # Detect if the away step was a dropstep\n",
    "        # done by seeing if any of the alphas where zeroed out.\n",
    "        i = 0\n",
    "        while i < len(self.A_t):\n",
    "            alpha_i = self.A_t[i]\n",
    "            if alpha_i <= 0:\n",
    "                debug_info['drop_step'] = 'AS'\n",
    "                self.A_t.pop(i)\n",
    "                self.S_t.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        debug_info['v_t_idx'] = v_t_idx\n",
    "        if debug:\n",
    "            info.update(debug_info)\n",
    "        return self.S_t, self.A_t, info\n",
    "\n",
    "    def fw_step_away(self, x_t, g_t, debug=True):\n",
    "        # alg from FW_varients.pdf\n",
    "        use_conv_comb_x_t = False\n",
    "        info = {}\n",
    "        debug_info = {}\n",
    "        \n",
    "        # FW direction\n",
    "        s_t = self.lmo.get(g_t)\n",
    "        d_t_FW = s_t - x_t\n",
    "        # AWAY direction. From set of vertices already visited\n",
    "        away_sign = 1\n",
    "        away_costs = []\n",
    "        for v in self.S_t:\n",
    "            away_costs.append(torch.sum(away_sign * g_t * v).item()) \n",
    "        v_t_idx = np.argmax(away_costs) \n",
    "        v_t = self.S_t[v_t_idx]\n",
    "        d_t_AWAY = x_t - v_t\n",
    "        # check optimality (FW gap)\n",
    "        gap_FW = torch.sum(-g_t * d_t_FW).item()\n",
    "        gap_AWAY = torch.sum(-g_t * d_t_AWAY).item()\n",
    "        info['gap_FW'] = gap_FW\n",
    "        info['gap_AS'] = gap_AWAY\n",
    "        debug_info['awayCosts'] = away_costs\n",
    "\n",
    "        # check which direction is closer to the -gradient\n",
    "        if (gap_FW >= gap_AWAY) or (len(self.S_t) == 1): # don't step away if only one vertex in S_t\n",
    "            step_type = 'FW'\n",
    "            d_t = d_t_FW\n",
    "            max_step = 1\n",
    "        else:\n",
    "            step_type = 'AS'\n",
    "            d_t = d_t_AWAY\n",
    "            alpha_v_t = self.A_t[v_t_idx]\n",
    "            max_step = 1 if alpha_v_t == 1 else alpha_v_t / (1 - alpha_v_t)  # avoid divide by zero when alpha = 1\n",
    "        self.d_t = d_t\n",
    "        info['step_type'] = step_type\n",
    "        debug_info['max_step'] = max_step\n",
    "        # determine stepsize according to rule\n",
    "        fw_stepsize = self.stepsize_method.get_stepsize(x_t, d_t, max_step)\n",
    "        info['stepsize'] = fw_stepsize\n",
    "\n",
    "        self.S_t, self.A_t, update_info = self.update_active_away(fw_stepsize, s_t, v_t_idx, step_type,\n",
    "                                                debug=debug)\n",
    "\n",
    "        ## UPDATE x_t\n",
    "        perturbed_image_step = x_t + fw_stepsize * d_t\n",
    "        perturbed_image_alpha = sum([alpha * v for alpha, v in zip(self.A_t, self.S_t)])\n",
    "        if use_conv_comb_x_t: # use x_t = A_t.T * S_t or x_t + gamma * d_t ?\n",
    "            perturbed_image = perturbed_image_alpha\n",
    "        else:\n",
    "            perturbed_image = perturbed_image_step\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "        # LOGGING and DEBUG info\n",
    "        info['alphas'] = self.A_t\n",
    "        debug_info['L_inf_step'] = torch.max(torch.abs(perturbed_image_step - self.x0_denorm)).item()\n",
    "        debug_info['L_inf_alpha'] = torch.max(torch.abs(perturbed_image_alpha - self.x0_denorm)).item()\n",
    "        alpha_np = ((perturbed_image_alpha).squeeze(0).permute(1, 2, 0).numpy()).clip(0,1)\n",
    "        step_np = ((perturbed_image_step).squeeze(0).permute(1, 2, 0).numpy()).clip(0,1)\n",
    "        debug_info['step_alpha_diffFactor'] = (alpha_np - step_np).sum() / self.epsilon\n",
    "        info.update(update_info)\n",
    "        if debug:\n",
    "            info.update(debug_info)\n",
    "        self.last_d = d_t\n",
    "        return perturbed_image, gap_FW, info\n",
    "\n",
    "    def fw_step_away_m(self, x_t, g_t, debug=True, momentum=0.8):\n",
    "        # alg from FW_varients.pdf\n",
    "        use_conv_comb_x_t = False\n",
    "        info = {}\n",
    "        debug_info = {}\n",
    "        \n",
    "        # FW direction\n",
    "        self.m_t = (1 - momentum) * g_t + momentum * self.m_t\n",
    "        s_t = self.lmo.get(self.m_t)\n",
    "        d_t_FW = s_t - x_t\n",
    "        # AWAY direction. From set of vertices already visited\n",
    "        away_sign = 1\n",
    "        away_costs = []\n",
    "        for v in self.S_t:\n",
    "            away_costs.append(torch.sum(away_sign * g_t * v).item()) \n",
    "        v_t_idx = np.argmax(away_costs) \n",
    "        v_t = self.S_t[v_t_idx]\n",
    "        d_t_AWAY = x_t - v_t\n",
    "        # check optimality (FW gap)\n",
    "        gap_FW = torch.sum(-g_t * d_t_FW).item()\n",
    "        gap_AWAY = torch.sum(-g_t * d_t_AWAY).item()\n",
    "        info['gap_FW'] = gap_FW\n",
    "        info['gap_AS'] = gap_AWAY\n",
    "        debug_info['awayCosts'] = away_costs\n",
    "\n",
    "        # check which direction is closer to the -gradient\n",
    "        if (gap_FW >= gap_AWAY) or (len(self.S_t) == 1): # don't step away if only one vertex in S_t\n",
    "            step_type = 'FW'\n",
    "            d_t = d_t_FW\n",
    "            max_step = 1\n",
    "        else:\n",
    "            step_type = 'AS'\n",
    "            d_t = d_t_AWAY\n",
    "            alpha_v_t = self.A_t[v_t_idx]\n",
    "            max_step = 1 if alpha_v_t == 1 else alpha_v_t / (1 - alpha_v_t)  # avoid divide by zero when alpha = 1\n",
    "        self.d_t = d_t\n",
    "        info['step_type'] = step_type\n",
    "        debug_info['max_step'] = max_step\n",
    "        # determine stepsize according to rule\n",
    "        fw_stepsize = self.stepsize_method.get_stepsize(self.stepsize_method.x_t_denorm, d_t, max_step)\n",
    "        info['stepsize'] = fw_stepsize\n",
    "\n",
    "        self.S_t, self.A_t, update_info = self.update_active_away(fw_stepsize, s_t, v_t_idx, step_type,\n",
    "                                                debug=debug)\n",
    "\n",
    "        ## UPDATE x_t\n",
    "        perturbed_image_step = x_t + fw_stepsize * d_t\n",
    "        perturbed_image_alpha = sum([alpha * v for alpha, v in zip(self.A_t, self.S_t)])\n",
    "        if use_conv_comb_x_t: # use x_t = A_t.T * S_t or x_t + gamma * d_t ?\n",
    "            perturbed_image = perturbed_image_alpha\n",
    "        else:\n",
    "            perturbed_image = perturbed_image_step\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "        # LOGGING and DEBUG info\n",
    "        info['alphas'] = self.A_t\n",
    "        debug_info['L_inf_step'] = torch.max(torch.abs(perturbed_image_step - self.x0_denorm)).item()\n",
    "        debug_info['L_inf_alpha'] = torch.max(torch.abs(perturbed_image_alpha - self.x0_denorm)).item()\n",
    "        alpha_np = ((perturbed_image_alpha).squeeze(0).permute(1, 2, 0).numpy()).clip(0,1)\n",
    "        step_np = ((perturbed_image_step).squeeze(0).permute(1, 2, 0).numpy()).clip(0,1)\n",
    "        debug_info['step_alpha_diffFactor'] = (alpha_np - step_np).sum() / self.epsilon\n",
    "        info.update(update_info)\n",
    "        if debug:\n",
    "            info.update(debug_info)\n",
    "        self.last_d = d_t\n",
    "        return perturbed_image, gap_FW, info\n",
    "\n",
    "    def update_active_pair(self, gamma, s_t, v_t_idx, info):\n",
    "        drop = True\n",
    "        diffs = [torch.sum(torch.abs(s_t - s)).item() for s in self.S_t]  # [torch.max(torch.abs(s_t - s)).item() for s in S_t]\n",
    "        min_diff = min(diffs)\n",
    "        arg = np.argmin(diffs)\n",
    "        if min_diff < 0.9 * self.epsilon:\n",
    "            # s_t already in S_t\n",
    "            s_t_idx = arg\n",
    "        else:\n",
    "            self.S_t.append(s_t)\n",
    "            self.A_t.append(0.0)\n",
    "            s_t_idx = -1\n",
    "        #self.A_t = [a + gamma if i == s_t_idx else a - gamma for i, a in enumerate(self.A_t)]\n",
    "        self.A_t[s_t_idx] += gamma\n",
    "        self.A_t[v_t_idx] -= gamma\n",
    "        \n",
    "        tol = 0.001\n",
    "        i=0\n",
    "        while i < len(self.A_t):\n",
    "            alpha_v_i = self.A_t[i]\n",
    "            s_t_i = self.S_t[i]\n",
    "            if abs(1 - alpha_v_i) < tol:\n",
    "                self.A_t = [1.0]\n",
    "                self.S_t = [s_t_i]\n",
    "                break\n",
    "            elif self.A_t[i] < tol:\n",
    "                if drop:\n",
    "                    self.A_t.pop(i)\n",
    "                    self.S_t.pop(i)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                i += 1\n",
    "        info['A_t'] = copy.deepcopy(self.A_t)\n",
    "        return self.S_t, self.A_t\n",
    "\n",
    "    def fw_step_pairwise(self, x_t, g_t):\n",
    "        use_conv_comb_x_t = True\n",
    "        info = {}\n",
    "        debug_info = {}\n",
    "\n",
    "        # Using LMO to compute s_t\n",
    "        s_t = self.lmo.get(g_t)\n",
    "        d_t_FW = s_t - x_t\n",
    "\n",
    "        # AWAY direction. From set of vertices already visited\n",
    "        away_costs = []\n",
    "        for v in self.S_t:\n",
    "            away_costs.append(torch.sum(g_t * v).item())\n",
    "        v_t_idx = np.argmax(away_costs)\n",
    "        v_t = self.S_t[v_t_idx]\n",
    "        alpha_v_t = self.A_t[v_t_idx]\n",
    "        max_step = alpha_v_t\n",
    "        d_t_AWAY = x_t - v_t\n",
    "        self.away_costs = away_costs\n",
    "\n",
    "        gap_FW = torch.sum(-g_t * d_t_FW).item()\n",
    "        gap_AWAY = torch.sum(-g_t * d_t_AWAY).item()\n",
    "        info['gap_FW'] = gap_FW\n",
    "        info['gap_AS'] = gap_AWAY\n",
    "\n",
    "        d_t = d_t_FW + d_t_AWAY #s_t - v_t\n",
    "        self.d_t = d_t\n",
    "        fw_stepsize = self.stepsize_method.get_stepsize(x_t, d_t, max_step)\n",
    "\n",
    "        self.S_t, self.A_t = self.update_active_pair(fw_stepsize, s_t, v_t_idx, info)\n",
    "        perturbed_image_step = x_t + fw_stepsize * d_t\n",
    "        perturbed_image_alpha = sum([alpha * v for alpha, v in zip(self.A_t, self.S_t)])\n",
    "        if use_conv_comb_x_t: \n",
    "            perturbed_image = perturbed_image_alpha\n",
    "        else:\n",
    "            perturbed_image = perturbed_image_step\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "        info.update(debug_info)\n",
    "\n",
    "        return perturbed_image, gap_FW, info\n",
    "    \n",
    "    def update_active_pair_test(self, gamma, s_t, v_t_idx, debug = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gamma (float): stepsize\n",
    "            gamma_max (int): Max stepsize informs when FW step will make S_t singular or AS dropstep\n",
    "            S_t (list(torch.Tensor)): Active set of directions s.t. x_t in conv{S_t}\n",
    "            A_t (list(float)): coefficients corresponding to atoms in S_t. x_t = A_t .* S_t\n",
    "            v_t_idx (int): index of away atom in S_t\n",
    "        \"\"\"\n",
    "        info = {}\n",
    "        debug_info = {}\n",
    "        if abs(gamma - 1) < 0.0001:\n",
    "            # drop step\n",
    "            self.S_t = [s_t]\n",
    "            self.A_t = [1.0]\n",
    "            debug_info['drop_step'] = 'FW'\n",
    "        else:\n",
    "            ## UPDATE S\n",
    "            # need to check if vertex is already in S\n",
    "            diffs = [torch.sum(torch.abs(s_t - s)).item() for s in self.S_t]\n",
    "            min_diff = min(diffs)\n",
    "            arg = np.argmin(diffs)\n",
    "            debug_info['min_revisit_diff'] = min_diff\n",
    "            if min_diff < 0.9*self.epsilon:\n",
    "                # s_t already in S_t\n",
    "                s_t_idx = arg\n",
    "                debug_info['FW_revisit'] = True\n",
    "            else:\n",
    "                self.S_t.append(s_t)\n",
    "                self.A_t.append(0.0)\n",
    "                s_t_idx = -1\n",
    "            #debug_info[\"min_revisit_diff\"] = min_diff\n",
    "            \n",
    "            ## UPDATE ALPHAS\n",
    "            #self.A_t = [(1 - gamma) * alpha for alpha in self.A_t]\n",
    "            self.A_t[s_t_idx] += gamma\n",
    "            if False: #gamma >= gamma_max:\n",
    "                # drop step: remove atom and alpha\n",
    "                # logic changed to check if alpha is zeroed (see below)\n",
    "                self.A_t.pop(v_t_idx)\n",
    "                self.S_t.pop(v_t_idx)\n",
    "                debug_info['drop_step'] = 'AS'\n",
    "            else:\n",
    "                ## UPDATE ALPHAS\n",
    "                #self.A_t = [(1 + gamma) * alpha for alpha in self.A_t]\n",
    "                self.A_t[v_t_idx] -= gamma\n",
    "        \n",
    "        # Detect if the away step was a dropstep\n",
    "        # done by seeing if any of the alphas where zeroed out.\n",
    "        i = 0\n",
    "        while i < len(self.A_t):\n",
    "            alpha_i = self.A_t[i]\n",
    "            if (alpha_i <= 0) and (len(self.A_t) > 1):\n",
    "                debug_info['drop_step'] = 'AS'\n",
    "                self.A_t.pop(i)\n",
    "                self.S_t.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        debug_info['v_t_idx'] = v_t_idx\n",
    "        if debug:\n",
    "            info.update(debug_info)\n",
    "        return self.S_t, self.A_t, info\n",
    "    \n",
    "    def fw_step_pairwise_test(self, x_t, g_t, debug=True):\n",
    "        # alg from FW_varients.pdf\n",
    "        use_conv_comb_x_t = True\n",
    "        info = {}\n",
    "        debug_info = {}\n",
    "        \n",
    "        # FW direction\n",
    "        s_t = self.lmo.get(g_t)\n",
    "        d_t_FW = s_t - x_t\n",
    "        # AWAY direction. From set of vertices already visited\n",
    "        away_sign = 1\n",
    "        away_costs = []\n",
    "        for v in self.S_t:\n",
    "            away_costs.append(torch.sum(away_sign * g_t * v).item()) \n",
    "        v_t_idx = np.argmax(away_costs) \n",
    "        v_t = self.S_t[v_t_idx]\n",
    "        d_t_AWAY = x_t - v_t\n",
    "        # check optimality (FW gap)\n",
    "        gap_FW = torch.sum(-g_t * d_t_FW).item()\n",
    "        gap_AWAY = torch.sum(-g_t * d_t_AWAY).item()\n",
    "        info['gap_FW'] = gap_FW\n",
    "        info['gap_AS'] = gap_AWAY\n",
    "        debug_info['awayCosts'] = away_costs\n",
    "\n",
    "        # # check which direction is closer to the -gradient\n",
    "        # if (gap_FW >= gap_AWAY) or (len(self.S_t) == 1): # don't step away if only one vertex in S_t\n",
    "        #     step_type = 'FW'\n",
    "        #     d_t = d_t_FW\n",
    "        #     max_step = 1\n",
    "        # else:\n",
    "        #     step_type = 'AS'\n",
    "        #     d_t = d_t_AWAY\n",
    "        #     alpha_v_t = self.A_t[v_t_idx]\n",
    "        #     max_step = 1 if alpha_v_t == 1 else alpha_v_t / (1 - alpha_v_t)  # avoid divide by zero when alpha = 1\n",
    "        # info['step_type'] = step_type\n",
    "        # debug_info['max_step'] = max_step\n",
    "        # DEFINE PAIRWISE DIRECTION\n",
    "        d_t = s_t - v_t\n",
    "        alpha_v_t = self.A_t[v_t_idx]\n",
    "        max_step = alpha_v_t\n",
    "        info['max_stepsize'] = max_step\n",
    "        # determine stepsize according to rule\n",
    "        fw_stepsize = self.stepsize_method.get_stepsize(x_t, d_t, max_step)\n",
    "        info['stepsize'] = fw_stepsize\n",
    "\n",
    "        self.S_t, self.A_t, update_info = self.update_active_pair_test(fw_stepsize, s_t, v_t_idx,\n",
    "                                                debug=debug)\n",
    "\n",
    "        ## UPDATE x_t\n",
    "        perturbed_image_step = x_t + fw_stepsize * d_t\n",
    "        perturbed_image_alpha = sum([alpha * v for alpha, v in zip(self.A_t, self.S_t)])\n",
    "        if use_conv_comb_x_t: # use x_t = A_t.T * S_t or x_t + gamma * d_t ?\n",
    "            perturbed_image = perturbed_image_alpha\n",
    "        else:\n",
    "            perturbed_image = perturbed_image_step\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "        # LOGGING and DEBUG info\n",
    "        info['alphas'] = self.A_t\n",
    "        debug_info['L_inf_step'] = torch.max(torch.abs(perturbed_image_step - self.x0_denorm)).item()\n",
    "        debug_info['L_inf_alpha'] = torch.max(torch.abs(perturbed_image_alpha - self.x0_denorm)).item()\n",
    "        alpha_np = ((perturbed_image_alpha).squeeze(0).permute(1, 2, 0).numpy()).clip(0,1)\n",
    "        step_np = ((perturbed_image_step).squeeze(0).permute(1, 2, 0).numpy()).clip(0,1)\n",
    "        debug_info['step_alpha_diffFactor'] = (alpha_np - step_np).sum() / self.epsilon\n",
    "        info.update(update_info)\n",
    "        if debug:\n",
    "            info.update(debug_info)\n",
    "        self.last_d = d_t\n",
    "        return perturbed_image, gap_FW, info"
   ],
   "id": "93347c1263ff0d97",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Other Utils",
   "id": "d44454eb2ccf695c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:39:09.226631Z",
     "start_time": "2024-07-04T17:39:09.200624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LMO:\n",
    "    def __init__(self, epsilon, x0, p):\n",
    "        self.x0 = x0.clone().detach()  # Ensure x0 is not modified elsewhere\n",
    "        self.epsilon = epsilon\n",
    "        self.p = p\n",
    "        # Select the appropriate LMO method based on the norm p\n",
    "        if p == -1:\n",
    "            self.method = self._LMO_inf\n",
    "        elif p == 1:\n",
    "            self.method = self._LMO_l1\n",
    "        elif p == 2:\n",
    "            self.method = self._LMO_l2\n",
    "        else:\n",
    "            raise Exception(f\"invalid choice of norm {p}\")\n",
    "\n",
    "    def get(self, g_t):\n",
    "        return self.method(g_t)\n",
    "\n",
    "    def _LMO_inf(self, g_t):\n",
    "        g_t_sign = g_t.sign()  # Get the sign of the gradient\n",
    "        s_t = -self.epsilon * g_t_sign + self.x0  # Update step for l_inf norm\n",
    "        return s_t\n",
    "\n",
    "    def _LMO_l1(self, gradient):\n",
    "        abs_gradient = gradient.abs()  # Get the absolute value of the gradient\n",
    "        sign_gradient = gradient.sign()  # Get the sign of the gradient\n",
    "        perturbation = torch.zeros_like(gradient)\n",
    "        # For each example in the batch, select the component with the maximum absolute gradient\n",
    "        for i in range(gradient.size(0)):\n",
    "            _, idx = torch.topk(abs_gradient[i].view(-1), 1)\n",
    "            perturbation[i].view(-1)[idx] = sign_gradient[i].view(-1)[idx]\n",
    "        return self.epsilon * perturbation\n",
    "\n",
    "    def _LMO_l2(self, g_t): # from arxiv version of attacks.pdf\n",
    "        g_t_norm = torch.norm(g_t, p=2, dim=-1, keepdim=True)\n",
    "        s_t = -self.epsilon * g_t / g_t_norm + self.x0\n",
    "        return s_t\n",
    "\n",
    "class AdversarialLoss(nn.Module):\n",
    "    def __init__(self, num_classes, specific_label=None):\n",
    "        \"\"\"\n",
    "        Initialize the AdversarialLoss.\n",
    "        \n",
    "        Args:\n",
    "        - num_classes (int): Total number of classes in the classification problem.\n",
    "        - specific_label (int, optional): A specific incorrect label to target. If None, the loss will consider all incorrect labels.\n",
    "        \"\"\"\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.specific_label = specific_label\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the adversarial loss.\n",
    "        \n",
    "        Args:\n",
    "        - outputs (torch.Tensor): The model outputs (logits) of shape (batch_size, num_classes).\n",
    "        - targets (torch.Tensor): The true labels of shape (batch_size,).\n",
    "        \n",
    "        Returns:\n",
    "        - loss (torch.Tensor): The computed adversarial loss.\n",
    "        \"\"\"\n",
    "        batch_size = outputs.size(0)\n",
    "        if self.specific_label is not None:\n",
    "            # Targeting a specific incorrect label\n",
    "            incorrect_labels = torch.full_like(targets, self.specific_label)\n",
    "            mask = (incorrect_labels != targets).float()\n",
    "            specific_log_probs = F.log_softmax(outputs, dim=1).gather(1, incorrect_labels.unsqueeze(1)).squeeze(1)\n",
    "            loss = -specific_log_probs * mask\n",
    "            return loss.mean()\n",
    "\n",
    "        # if self.specific_label is not None:\n",
    "        #     # Targeting a specific incorrect label\n",
    "        #     specific_labels = torch.full_like(targets, self.specific_label)\n",
    "            \n",
    "        #     # Compute log probabilities\n",
    "        #     log_probs = F.log_softmax(outputs, dim=1)\n",
    "            \n",
    "        #     # Get the log probabilities of the specific label\n",
    "        #     specific_log_probs = log_probs.gather(1, specific_labels.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "        #     # Compute the negative log likelihood\n",
    "        #     loss = -specific_log_probs\n",
    "        #     return loss.mean()\n",
    "        else:\n",
    "            # Averaging over all incorrect labels\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "            incorrect_log_probs = log_probs.clone()\n",
    "            correct_log_probs = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "            for i in range(batch_size):\n",
    "                incorrect_log_probs[i, targets[i]] = float('-inf')\n",
    "            average_incorrect_log_probs = incorrect_log_probs.logsumexp(dim=1) - torch.log(torch.tensor(self.num_classes - 1, dtype=torch.float))\n",
    "            loss = -average_incorrect_log_probs\n",
    "            return loss.mean()\n",
    "        \n",
    "    def forward1(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the adversarial loss.\n",
    "        \n",
    "        Args:\n",
    "        - outputs (torch.Tensor): The model outputs (logits) of shape (batch_size, num_classes).\n",
    "        - targets (torch.Tensor): The true labels of shape (batch_size,).\n",
    "        \n",
    "        Returns:\n",
    "        - loss (torch.Tensor): The computed adversarial loss.\n",
    "        \"\"\"\n",
    "        batch_size = outputs.size(0)\n",
    "        if self.specific_label is not None:\n",
    "            adv_target = torch.full((outputs.size(0),), self.specific_label, dtype=torch.long)\n",
    "            if isinstance(targets, int):\n",
    "                targets = torch.full((outputs.size(0),), targets, dtype=torch.long)\n",
    "            \n",
    "            return F.cross_entropy(outputs, adv_target)\n",
    "        else:\n",
    "            if isinstance(targets, int):\n",
    "                targets = torch.full((outputs.size(0),), targets, dtype=torch.long)\n",
    "            return -F.nll_loss(outputs, targets)\n",
    "        \n",
    "class stepsize():\n",
    "    def __init__(self, model, strat, x0, fixed_size = 1, ls_criterion=None, ls_target = None, ls_num_samples=50, renorm = None):\n",
    "        if isinstance(strat, (float, int)):\n",
    "            fixed_size = strat\n",
    "            strat = 'fixed'\n",
    "        self.model = model\n",
    "        self.strat = strat\n",
    "        self.fixed_size = fixed_size\n",
    "        # used for amjo\n",
    "        self.x0 = x0\n",
    "        self.x_t_grad = None\n",
    "        self.loss0 = None\n",
    "\n",
    "        # used for ls\n",
    "        self.ls_criterion = ls_criterion\n",
    "        self.ls_target = ls_target\n",
    "        self.ls_num_samples = ls_num_samples\n",
    "        self.renorm = renorm\n",
    "        \n",
    "        self.L_MNIST = L_MNIST\n",
    "        self.L_FMNIST = L_FMNIST\n",
    "        self.L_CIFAR = L_CIFAR\n",
    "\n",
    "        if self.strat == 'lipschitz_mnist':\n",
    "            self.stepsize = 1 / self.L_MNIST\n",
    "        elif self.strat == 'lipschitz_fmnist':\n",
    "            self.stepsize = 1 / self.L_FMNIST\n",
    "        elif self.strat == 'lipschitz_cifar':\n",
    "            self.stepsize = 1 / self.L_CIFAR\n",
    "\n",
    "        self.stepsize = fixed_size # will be updated if using other method\n",
    "        if self.strat not in ['fixed', 'ls', 'decay', 'amjo', 'lipschitz_mnist', 'lipschitz_fmnist', 'lipschitz_cifar']:\n",
    "            raise Exception(\"Accepted stepsize rules are ['fixed', 'ls', 'decay', 'amjo', 'lipschitz_mnist', 'lipschitz_fmnist', 'lipschitz_cifar']\")\n",
    "    \n",
    "    def set_stepsize_decay(self, t):\n",
    "        self.stepsize = 2 / (t + 2)\n",
    "        return\n",
    "\n",
    "    def stepsize_linesearch(self, x_t, d_t, max_step = 1):\n",
    "        x_tc = x_t.clone().detach()\n",
    "        d_tc = d_t.clone().detach()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            steps = [max_step * (i + 1) / self.ls_num_samples for i in range(self.ls_num_samples)]\n",
    "            self._sizes_ls = steps\n",
    "            for step in steps:\n",
    "                new_x = self.renorm(x_tc + step * d_tc)\n",
    "                output = self.model(new_x)\n",
    "                loss = self.ls_criterion(output, self.ls_target).item()\n",
    "                losses.append(loss)\n",
    "        best_idx = np.argmin(losses)\n",
    "        self.stepsize = steps[best_idx]\n",
    "        \n",
    "        self._loss_ls = losses\n",
    "        return self.stepsize\n",
    "    \n",
    "\n",
    "    # def stepsize_linesearch(self, x_t, d_t, max_step=1):\n",
    "    #     x_tc = x_t.clone().detach()\n",
    "    #     d_tc = d_t.clone().detach()\n",
    "    #     losses = []\n",
    "    #     with torch.no_grad():\n",
    "    #         # Create a tensor of steps\n",
    "    #         steps = torch.linspace(max_step / self.ls_num_samples, max_step, self.ls_num_samples).to(x_t.device).view(-1, 1, 1, 1)\n",
    "\n",
    "    #         # Generate all possible x_t + step * d_t combinations in parallel\n",
    "    #         x_t_steps = x_tc + steps * d_tc\n",
    "\n",
    "    #         # Flatten the batch and step dimensions for efficient processing\n",
    "    #         batch_size = x_t.size(0)\n",
    "    #         num_steps = steps.size(0)\n",
    "    #         #x_t_steps = x_t_steps.view(batch_size * num_steps, *x_t.size()[1:])\n",
    "\n",
    "    #         # Pass through the model in parallel\n",
    "    #         output = self.model(x_t_steps)\n",
    "\n",
    "    #         # # Compute losses\n",
    "    #         output = output.view(batch_size, num_steps, -1)\n",
    "    #         losses = []\n",
    "    #         for out in output:\n",
    "    #             losses.append(self.ls_criterion(out, self.ls_target).item())\n",
    "            \n",
    "    #         # Find the index of the minimum loss for each example in the batch\n",
    "    #         best_idx = np.argmin(losses)\n",
    "            \n",
    "    #         # Get the corresponding steps\n",
    "    #         self.stepsize = steps[best_idx].item()\n",
    "            \n",
    "    #     return self.stepsize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def stepsize_armijo(self, x_t, d_t, max_step = 1):\n",
    "        info_step = {}\n",
    "        x_tc = x_t.clone().detach()\n",
    "        d_tc = d_t.clone().detach()\n",
    "        step_size = max_step\n",
    "        best_stepsize = max_step\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "        initial_loss = self.ls_criterion(self.model(self.renorm(x_tc)), self.ls_target)#F.cross_entropy(self.model(x_k), target).item()\n",
    "        min_loss = float('inf')\n",
    "        \n",
    "        while step_size > 1e-4:\n",
    "            new_point = self.renorm(x_tc + step_size * d_tc)\n",
    "            new_loss = self.ls_criterion(self.model(new_point), self.ls_target)#F.cross_entropy(self.model(new_point), target).item()\n",
    "            RHS = initial_loss + gamma * step_size * torch.sum(self.x_t_grad * d_tc).item()\n",
    "            if new_loss < min_loss:\n",
    "                min_loss = new_loss\n",
    "                best_stepsize = step_size\n",
    "            if new_loss <= RHS:\n",
    "                return step_size\n",
    "            \n",
    "            step_size *= delta\n",
    "        \n",
    "        return best_stepsize\n",
    "\n",
    "\n",
    "    def get_stepsize(self, x_t, d_t, max_step = 1):\n",
    "        if self.strat == 'ls':\n",
    "            #fw_stepsize = self.exact_ls(x_t, d_t, max_step)\n",
    "            fw_stepsize = self.stepsize_linesearch(x_t, d_t, max_step)\n",
    "        elif self.strat == 'amjo':\n",
    "            fw_stepsize = self.stepsize_armijo(x_t, d_t, max_step)\n",
    "        else:\n",
    "            fw_stepsize = self.stepsize\n",
    "        fw_stepsize = min(fw_stepsize, max_step)\n",
    "        return fw_stepsize\n",
    "    \n",
    "def early_stopper(criterion, t, success, first_success, info, gap_FW_tol, max_fw_iter, gap_FW):\n",
    "    new_correct = 0\n",
    "    if (criterion == 'pred') and first_success:\n",
    "        # The attack was successful so the classification was not correct\n",
    "        stop = True\n",
    "        stop_reason = 'pred'\n",
    "    elif (criterion == 'gap_FW') and (gap_FW < gap_FW_tol):\n",
    "        if not success: # attack failed\n",
    "            new_correct +=1\n",
    "        stop = True\n",
    "        stop_reason = 'gap'\n",
    "    elif (t == max_fw_iter - 1): # Stop condition: Hit max FW iters\n",
    "        stop = True\n",
    "        stop_reason = 'max_iter'\n",
    "        if not success: # attack failed\n",
    "            new_correct +=1\n",
    "    elif (criterion == 'gap_pairwise') and (info is not None) and (info['gap_pairwise'] < gap_FW_tol):\n",
    "        if not success: # attack failed\n",
    "            new_correct +=1\n",
    "        stop = True\n",
    "        stop_reason = 'gap_pairwise'\n",
    "    else:\n",
    "        # no stop criteria met, continue\n",
    "        stop = False\n",
    "        stop_reason = None\n",
    "    return stop, stop_reason, new_correct\n",
    "\n",
    "class example_saver():\n",
    "    def __init__(self, num_adv_ex = 10, num_failed_ex = 10) -> None:\n",
    "        self.num_adv_ex = num_adv_ex\n",
    "        self.num_failed_ex = num_failed_ex\n",
    "        self.adv_true = []\n",
    "        self.adv_pred = []\n",
    "        self.adv_x0 = []\n",
    "        self.adv_atk = []\n",
    "        self.adv_xt = []\n",
    "        self.adv_true_init_prob = []\n",
    "        self.adv_final_prob = []\n",
    "        self.adv_ex = []\n",
    "        self.failed_ex = []\n",
    "        self.info = []\n",
    "        pass\n",
    "\n",
    "    def save_ex(self, perturbed_image, x0, true, final_pred, success, true_class_prob0, pred_class_prob):\n",
    "        self.info.append(true)\n",
    "        \n",
    "        if (len(self.adv_ex) >= self.num_adv_ex) and (len(self.failed_ex) >= self.num_failed_ex):\n",
    "            return\n",
    "\n",
    "        atk = perturbed_image - x0\n",
    "        atk = atk.detach().squeeze().cpu().numpy()\n",
    "        x0 = x0.detach().squeeze().cpu().numpy()\n",
    "        ex = perturbed_image.squeeze().detach().cpu().numpy()\n",
    "        if len(atk.shape) > 2:\n",
    "            atk = np.transpose(atk, (1, 2, 0))\n",
    "            x0 = np.transpose(x0, (1, 2, 0))\n",
    "            ex = np.transpose(ex, (1, 2, 0))\n",
    "\n",
    "\n",
    "        # Save some adv examples for visualization later\n",
    "        if success and (len(self.adv_ex) < self.num_adv_ex):\n",
    "            self.adv_ex.append( (true.item(), final_pred.item(), ex) )\n",
    "            self.adv_true.append(true.item())\n",
    "            self.adv_pred.append(final_pred.item())\n",
    "            self.adv_x0.append(x0)\n",
    "            self.adv_atk.append(atk)\n",
    "            self.adv_true_init_prob.append(true_class_prob0)\n",
    "            self.adv_final_prob.append(pred_class_prob)\n",
    "            self.adv_xt.append(ex)\n",
    "        if (not success) and (len(self.failed_ex) < self.num_failed_ex):\n",
    "            self.failed_ex.append( (true.item(), final_pred.item(), ex) )\n",
    "\n",
    "\n",
    "def test(target_model, device, epsilon,num_fw_iter, num_test = 1000, method='fw', early_stopping = None, fw_stepsize_rule = 1, gap_FW_tol = 0.05, targeted = False, ex_saver=None, norm_p=-1, seed=42):\n",
    "    start = time.process_time()\n",
    "    target_model.query_count = 0\n",
    "    testloader = target_model.remake_testloader(seed)\n",
    "    model = target_model.model\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    hist = []\n",
    "    ex_num = 0\n",
    "    # Loop over all examples in test set\n",
    "    for x0, target in tqdm(testloader):\n",
    "        x_t = x0.detach().clone().to(device)  # Clone and move to device\n",
    "        # Send the data and label to the device\n",
    "        x0, target = x0.to(device), target.to(device)\n",
    "        x0_denorm = target_model.denorm(x0)\n",
    "        if targeted:\n",
    "            # select a random target for attack that is not the true target.\n",
    "            adv_target = random.randint(0, target_model.num_classes - 2)\n",
    "            adv_target = adv_target if adv_target < target else adv_target + 1\n",
    "            criterion = AdversarialLoss(target_model.num_classes, specific_label=adv_target)\n",
    "        else:\n",
    "            criterion = AdversarialLoss(target_model.num_classes)\n",
    "        lmo = LMO(epsilon, x0_denorm, norm_p)\n",
    "        stepsize_method = stepsize(model, fw_stepsize_rule, x0_denorm, ls_criterion=criterion, ls_target=target, renorm = target_model.renorm)\n",
    "        attackStep = AttackStep(method, epsilon, x0_denorm, lmo, stepsize_method)\n",
    "        #x_t.requires_grad = True  #Set requires_grad attribute of tensor. Important for Attack\n",
    "        had_first_success = False\n",
    "        gap_FW = None\n",
    "        info = None\n",
    "        true_class_prob0 = 0\n",
    "\n",
    "        for t in range(num_fw_iter):\n",
    "            # Step size calculation\n",
    "            if stepsize_method.strat == 'decay':\n",
    "                stepsize_method.set_stepsize_decay(t)\n",
    "            x_t.requires_grad = True\n",
    "            # Forward pass the data through the model\n",
    "            output = model(x_t)\n",
    "            target_model.query_count += 1\n",
    "            class_probs = torch.softmax(output,dim=1)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            if t==0:\n",
    "                # save init confidence in true class\n",
    "                true_class_prob0 = class_probs[0, target.item()].item()\n",
    "                stepsize_method.loss0 = loss.item()\n",
    "            init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "            # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "            if (init_pred.item() != target.item()) and (t == 0):\n",
    "                ex_num -= 1 # don't count this example\n",
    "                break\n",
    "            \n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            # Calculate gradients of model in backward pass\n",
    "            loss.backward()\n",
    "            x_t_grad = x_t.grad#.data\n",
    "            # Restore the data to its original scale\n",
    "            x_t_denorm = target_model.denorm(x_t)\n",
    "\n",
    "            # save information needed for linesearching stepsize rules\n",
    "            stepsize_method.x_t_grad = x_t_grad.clone().detach()\n",
    "\n",
    "            # Call Attack\n",
    "            with torch.no_grad():\n",
    "                perturbed_image, gap_FW, info = attackStep.step(x_t_denorm, x_t_grad)\n",
    "            \n",
    "            # Reapply normalization\n",
    "            x_t = target_model.renorm(perturbed_image)#transforms.Normalize((0.1307,), (0.3081,))(perturbed_image).detach()\n",
    "            # Re-classify the perturbed image\n",
    "            x_t.requires_grad = False\n",
    "            output = model(x_t)\n",
    "            info['l_inf'] = torch.max(torch.abs(x0_denorm - perturbed_image)).item()\n",
    "            info['mdlLoss'] = loss.item()\n",
    "\n",
    "            # Check for success\n",
    "            final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            if final_pred.item() == target.item():\n",
    "                success = False\n",
    "                first_success = False\n",
    "            else:\n",
    "                first_success =  not had_first_success\n",
    "                had_first_success = True\n",
    "                success = True\n",
    "            stop, stop_reason, new_correct = early_stopper(early_stopping, t, success, first_success, info, gap_FW_tol, num_fw_iter, gap_FW)\n",
    "            correct += new_correct\n",
    "\n",
    "            # metric logging\n",
    "            hist_iter = {\n",
    "                'example_idx':ex_num,\n",
    "                'FW_iter': t + 1, # original example is 0\n",
    "                'gap_FW': gap_FW if gap_FW is not None else None,\n",
    "                'success': success,\n",
    "                'first_success': first_success,\n",
    "                'target': target.item(),\n",
    "                'pred': final_pred.item(),\n",
    "                'stop_cond': stop_reason\n",
    "            }\n",
    "            if targeted:\n",
    "                hist_iter['adv_target'] = adv_target\n",
    "                targeted_success = (final_pred.item() == adv_target)\n",
    "                info['targeted_success'] = targeted_success\n",
    "            else:\n",
    "                targeted_success = False\n",
    "        \n",
    "            if info is not None:\n",
    "                hist_iter.update(info) # some methods output dict containing info at each step\n",
    "            hist.append(hist_iter)\n",
    "            if stop:\n",
    "                class_probs = torch.softmax(output,dim=1)\n",
    "                pred_class_prob = class_probs[0, final_pred.item()].item()\n",
    "                if ex_saver is not None:\n",
    "                    save_as_adv = targeted_success if targeted else success\n",
    "                    ex_saver.save_ex(perturbed_image, x0_denorm, target, final_pred, save_as_adv, true_class_prob0, pred_class_prob)\n",
    "                break\n",
    "        ex_num += 1\n",
    "        if ex_num >= num_test: # limit test set for speed\n",
    "            break\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/num_test\n",
    "    end = time.process_time()\n",
    "    print(f\"Epsilon: {epsilon}\\tCorrect Classifications (Failed Attacks) = {correct} / {num_test} = {final_acc}\" + f\"\\tTime: {end - start:.2f}s\")\n",
    "    print(f\"Total queries made: {target_model.query_count}\")\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples, pd.DataFrame(hist)\n",
    "\n",
    "def plot_convergence(hist_dfs, algs):\n",
    "    final_hist_dfs = [hist.groupby('example_idx').tail(1) for hist in hist_dfs]\n",
    "    for i, final_hist in enumerate(final_hist_dfs):\n",
    "        targeted = 'targeted_success' in final_hist.columns\n",
    "        ASR_text = 'Targeted Attack Success Rate' if targeted else 'Attack Success Rate'\n",
    "        alg = algs[i]\n",
    "        print(alg)\n",
    "        print(f\"\\t{ASR_text}: {final_hist['targeted_success' if targeted else 'success'].mean()}\")\n",
    "        print(f\"\\tAvg iters: {final_hist['FW_iter'].mean()}\")\n",
    "        if alg == 'fw_away':\n",
    "            st = hist_dfs[i].groupby('step_type').size().to_dict()\n",
    "            if len(st.keys()) > 1:\n",
    "                print(f\"\\tStep Types: FW {st['FW']}, AS {st['AS']}. {100 * st['AS'] / (st['FW'] + st['AS']):.1f}% Away Steps.\")\n",
    "            else:\n",
    "                print(\"\\t100% FW steps\")\n",
    "        plt.plot(hist_dfs[i].groupby('FW_iter')['gap_FW'].mean(), label=algs[i])\n",
    "    plt.xlabel(\"Iteration t\")\n",
    "    plt.ylabel(\"Average FW gap\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def display_examples(ex_saver, epsilon, classes, show_atk_mag = False, n_col = 3, offset = 2):\n",
    "    # classes either int corresponding to number of classes, or list with class names\n",
    "    if isinstance(classes, int):\n",
    "        classes = list(range(classes))\n",
    "    dim = len(ex_saver.adv_x0[0].shape)\n",
    "    \n",
    "    \n",
    "    n_col = min(n_col, len(ex_saver.adv_pred))\n",
    "    cmap = None if dim > 2 else 'gray'\n",
    "\n",
    "    fig, axs = plt.subplots(3,n_col)\n",
    "    for i in range(n_col):\n",
    "        ex_idx = i + offset\n",
    "        true = ex_saver.adv_true[ex_idx]\n",
    "        pred = ex_saver.adv_pred[ex_idx]\n",
    "        x0 = ex_saver.adv_x0[ex_idx]\n",
    "        atk = (ex_saver.adv_atk[ex_idx]+epsilon)/(2*epsilon)\n",
    "        if show_atk_mag:\n",
    "            atk = np.abs(atk-0.5)\n",
    "            atk *= 1/np.max(atk)\n",
    "        atk = np.clip(atk,0,1)\n",
    "        xt = ex_saver.adv_xt[ex_idx]\n",
    "        prob_true = ex_saver.adv_true_init_prob[ex_idx]\n",
    "        prob_adv = ex_saver.adv_final_prob[ex_idx]\n",
    "        axs[0, i].imshow(x0, cmap = cmap)\n",
    "        axs[0, i].axis('off')\n",
    "        axs[0, i].set_title(f\"Orginal: {classes[true]}\\np = {prob_true:.2f}\", fontsize=10)\n",
    "        axs[1, i].imshow(atk, cmap = cmap)\n",
    "        axs[1, i].axis('off')\n",
    "        axs[1, i].set_title(f\"Scaled offset:\\n = {epsilon}\", fontsize=10)\n",
    "        axs[2, i].imshow(xt, cmap = cmap)\n",
    "        axs[2, i].axis('off')\n",
    "        axs[2, i].set_title(f\"Adv Pred: {classes[pred]}\\np = {prob_adv:.2f}\", fontsize=10)\n",
    "    plt.tight_layout(pad=1.0, w_pad=-15, h_pad=1.0)\n",
    "    plt.show()"
   ],
   "id": "cf6ce18a825adcdf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8a5c22a5f2b6ab97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attacks",
   "id": "b70ec4a2b32fd927"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:39:15.886719Z",
     "start_time": "2024-07-04T17:39:12.863790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "hist_dfs = []\n",
    "final_hist_dfs = []\n",
    "hist = None\n",
    "\n",
    "targeted = False\n",
    "ex_saver = example_saver()\n",
    "\n",
    "algs = ['fw']\n",
    "for alg in algs:\n",
    "    acc, ex, hist = test(target_mdl, device,\n",
    "                         epsilon=0.1, \n",
    "                         num_fw_iter=20,\n",
    "                         method=alg, \n",
    "                         early_stopping='gap_FW', \n",
    "                         fw_stepsize_rule='lipschitz_mnist', \n",
    "                         gap_FW_tol=0.1,\n",
    "                         targeted=targeted, \n",
    "                         num_test=100, \n",
    "                         ex_saver=ex_saver,\n",
    "                         norm_p=2, \n",
    "                         seed=11)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "    hist_dfs.append(hist)\n",
    "    final_hist = hist.groupby('example_idx').tail(1)\n",
    "    final_hist_dfs.append(final_hist)\n",
    "final_hist_dfs[-1]"
   ],
   "id": "613414bbd40f0d2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 110/10000 [00:03<04:30, 36.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\tCorrect Classifications (Failed Attacks) = 52 / 100 = 0.52\tTime: 15.03s\n",
      "Total queries made: 2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      example_idx  FW_iter  gap_FW  success  first_success  target  pred  \\\n",
       "19              0       20     NaN     True          False       9     0   \n",
       "39              1       20     NaN    False          False       2     2   \n",
       "59              2       20     NaN    False          False       1     1   \n",
       "79              3       20     NaN    False          False       1     1   \n",
       "99              4       20     NaN     True          False       6     0   \n",
       "...           ...      ...     ...      ...            ...     ...   ...   \n",
       "1919           95       20     NaN    False          False       8     8   \n",
       "1939           96       20     NaN     True          False       5     0   \n",
       "1959           97       20     NaN     True          False       9     7   \n",
       "1979           98       20     NaN    False          False       4     4   \n",
       "1999           99       20     NaN     True          False       2     6   \n",
       "\n",
       "     stop_cond  stepsize  l_inf    mdlLoss  \n",
       "19    max_iter         1    NaN        NaN  \n",
       "39    max_iter         1    NaN   7.212705  \n",
       "59    max_iter         1    NaN  15.882296  \n",
       "79    max_iter         1    NaN  12.038165  \n",
       "99    max_iter         1    NaN   2.321671  \n",
       "...        ...       ...    ...        ...  \n",
       "1919  max_iter         1    NaN   4.413688  \n",
       "1939  max_iter         1    NaN        NaN  \n",
       "1959  max_iter         1    NaN   2.909183  \n",
       "1979  max_iter         1    NaN   3.211225  \n",
       "1999  max_iter         1    NaN   2.208971  \n",
       "\n",
       "[100 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_idx</th>\n",
       "      <th>FW_iter</th>\n",
       "      <th>gap_FW</th>\n",
       "      <th>success</th>\n",
       "      <th>first_success</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>stop_cond</th>\n",
       "      <th>stepsize</th>\n",
       "      <th>l_inf</th>\n",
       "      <th>mdlLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.212705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.882296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.038165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.321671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>95</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.413688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>96</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>97</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.909183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>98</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.211225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>max_iter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.208971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
